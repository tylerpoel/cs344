{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Homework 03\n",
    "\n",
    "## 3.1 - Restaurant Price Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Information gain is defined as: \n",
    "Gain(A) = Entropy(V) - Remainder(A)\n",
    "\n",
    "Remainder(Price) = 7/12 * Entropy(cheap) + 2/12 * Entropy(mid-level) + 3/12 * Entropy(expensive)\n",
    "\n",
    "Remainder(Price) = (7/12)*-1*((4/7)*lg(4/7)+(3/7)*lg(3/7)) + 0 + (3/12)*-1*((2/3)*lg(2/3)+(1/3)*lg(1/3))\n",
    "= 0.57 + 0 + 0.2 = 0.8.\n",
    "\n",
    "Therefore the Information Gain of Price is 1 - 0.8 = 0.2. \n",
    "\n",
    "This is more valuable than the type attribute (gain of 0), but less valuable than the \n",
    "patrons attribute (0.54 gain)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 - XOR Neural Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the neural network we created by hand in class, in was not possible to learn \n",
    "the XOR function, even though we were able to make it learn AND and OR. This is because\n",
    "AND and OR are learned through linear separation: in both cases a straight line can be drawn \n",
    "on the theoretical graph where the answers lie, and this line can cleanly separate \n",
    "the 1's and 0's, having them on each side of the line. The XOR function is not \n",
    "linearly separable, and therefore can't be learned by the Perceptron learning rule."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3 - Pandas"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Imports\n",
    "from keras.datasets import boston_housing\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Bringing in the data\n",
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3a - Compute the dimensions of the data structures"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Training data shape:\t (404, 13)\n",
      "Training targets shape:\t (404,)\n",
      "Testing data shape:\t (102, 13)\n",
      "Testing targets shape:\t (102,)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Displaying the dimensions of the data structures.\n",
    "print(\"Training data shape:\\t\", train_data.shape)\n",
    "print(\"Training targets shape:\\t\", train_targets.shape)\n",
    "print(\"Testing data shape:\\t\", test_data.shape)\n",
    "print(\"Testing targets shape:\\t\", test_targets.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3b - Construct training, test, and validation sets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Create data frames for the data\n",
    "df_train = pd.DataFrame(train_data)\n",
    "df_train[\"targets\"] = train_targets\n",
    "\n",
    "# Because of the way we loaded the data, our test set has already been created. \n",
    "test_set = pd.DataFrame(test_data)\n",
    "test_set[\"targets\"] = test_targets\n",
    "\n",
    "# Randomize the rows to prevent any bias\n",
    "df_train_new = df_train.reindex(np.random.permutation(df_train.index))\n",
    "\n",
    "# Splits the training data into a training set and a validation set\n",
    "training_set = df_train_new.head(325)\n",
    "validation_set = df_train_new.tail(79)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.c - Create a synthetic feature\n",
    "\n",
    "The synthetic feature I chose multiplies the Per capita crime rate and the\n",
    "% lower status of the population columns together, in hopes of creating a feature\n",
    "that can speak broadly to the socioeconomic health of a given community."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "df_train_new[\"Socioeconomic health\"] = df_train_new[0] * df_train_new[12]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}